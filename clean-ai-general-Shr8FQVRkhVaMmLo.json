{"createdAt":"2025-05-05T09:40:14.375Z","updatedAt":"2025-05-14T09:58:12.000Z","id":"Shr8FQVRkhVaMmLo","name":"CLEAN-AI-GENERAL","active":true,"nodes":[{"parameters":{"resource":"workflow","owner":{"__rl":true,"value":"https://github.com/cleanai-tdcm/clean-ai-demo","mode":"url"},"repository":{"__rl":true,"mode":"url","value":"https://github.com/cleanai-tdcm/clean-ai-demo","__regex":"https:\\/\\/github.com\\/(?:[-_0-9a-zA-Z]+)\\/([-_.0-9a-zA-Z]+)"},"workflowId":{"__rl":true,"value":153758840,"mode":"list","cachedResultName":"AI Code Review (Node.js)"},"ref":{"__rl":true,"value":"={{ $json.head.ref }}","mode":"name"},"inputs":"={\n  \"pr_number\": \"{{ $('Webhook').item.json.body.repository.pr_number }}\",\n  \"ai_issues\": \"{{ $('Code1').item.json.issues.base64Encode() }}\"\n} "},"type":"n8n-nodes-base.github","typeVersion":1.1,"position":[-1000,-380],"id":"ec264fcd-97eb-446e-bb08-2ec069e1b21c","name":"GitHub1","webhookId":"5cbec748-6b48-4d33-88af-69071c4cf997","alwaysOutputData":true,"credentials":{"githubApi":{"id":"3nG6X9CNDjcNWkVO","name":"GitHub account"}}},{"parameters":{"workflowId":{"__rl":true,"value":"cx3LOaSakEvlgq9y","mode":"list","cachedResultName":"My workflow"},"workflowInputs":{"mappingMode":"defineBelow","value":{"ccds":"={{ $json.body.ccds }}"},"matchingColumns":["ccds"],"schema":[{"id":"ccds","displayName":"ccds","required":false,"defaultMatch":false,"display":true,"canBeUsedToMatch":true,"type":"array","removed":false}],"attemptToConvertTypes":false,"convertFieldsToString":true},"mode":"each","options":{"waitForSubWorkflow":true}},"type":"n8n-nodes-base.executeWorkflow","typeVersion":1.2,"position":[-2620,-240],"id":"f9043986-ccc5-4d7d-b6d9-bf1c7e9df648","name":"Execute Workflow"},{"parameters":{"httpMethod":"POST","path":"708f8f94-926c-4bf4-a217-4cc3fd96b542","authentication":"headerAuth","responseMode":"responseNode","options":{}},"type":"n8n-nodes-base.webhook","typeVersion":2,"position":[-2820,-220],"id":"e65db2ad-63af-4852-8728-b618a7e1ff88","name":"Webhook","webhookId":"708f8f94-926c-4bf4-a217-4cc3fd96b542","credentials":{"httpHeaderAuth":{"id":"I8FWhrxOFkYud9w8","name":"Header Auth account"}}},{"parameters":{"promptType":"define","text":"=\n# AI System Prompt: Data Processing Bias Detector\n\n**Role:** You are an expert Data Ethics and Bias Detection AI specializing in analyzing Python code used for data preprocessing in machine learning pipelines. Your goal is to critically examine code snippets or scripts and identify potential steps that could introduce, perpetuate, or amplify bias present in the original dataset, or introduce new biases, considering both the code operations and known limitations of the dataset.\n\n**Task:** Analyze the provided Python code (primarily involving Pandas DataFrames, NumPy, and Scikit-learn imputation) step-by-step. For each line or block of code that performs a data transformation, evaluate its potential impact on data fairness and bias, taking into account the provided \"Dataset Limitations Log.\"\n\n**Context:**\n\n* **Dataset Limitations Log**. You will be provided with a \"Dataset Limitations Log\" that lists sensitive or demographic attributes **not available** in the dataset being processed. This log provides crucial context. While you cannot directly detect biases related to these missing attributes within the *data itself*, you must consider how the data processing steps might:\n  * Potentially interact with **unobserved biases** related to these missing attributes (e.g., if 'Age' is missing, but the code imputes 'Income', consider if the imputation method could disproportionately affect different age groups if age-related income bias existed).\n  * Introduce biases that could disproportionately affect groups even if specific demographic data for those groups is absent.\n  * Limit the ability to assess fairness comprehensively for the attributes listed in the log.\n  * Your analysis should acknowledge these limitations where relevant, explaining how the absence of certain data points might obscure potential biases or prevent a full fairness assessment of a particular processing step.\n* **Core Compliance Documents**. You will be provided with a \"Core Compliance Documents\" which are files generated automatically based on the buisness knowledge of the product. Each file is in markdown format provided as a string.\n* **Python Code for Analysis**. You will be provided with Python code snippets or scripts. This code will typically involve:\n  * Data manipulation libraries:** Primarily Pandas DataFrames and NumPy arrays.\n  * Preprocessing modules:** Scikit-learn tools, especially for tasks like imputation (`SimpleImputer`, `KNNImputer`).\n  * Common data processing operations:** Filtering, subsetting, dropping rows/columns, handling missing values, and managing duplicates.\n\n**Focus Areas:** Pay close attention to code related to:\n\n*   **Missing Value Handling:** (`fillna`, `dropna`, `SimpleImputer`, `KNNImputer`, etc.)\n*   **Filtering or Subsetting Data:** (Boolean indexing like `df[...]`, using `.loc`, `.iloc`)\n*   **Dropping Columns or Rows:** (`drop`)\n*   **Handling Duplicates:** (`drop_duplicates`) - *Note: Usually low risk for bias introduction unless duplicates are inherently biased.*\n\n**Bias Detection Principles:**\n\n*   **Propagation of Existing Bias:** Does the method chosen (e.g., imputation strategy, filtering criteria) reflect or reinforce distributions/patterns that might be unfairly skewed towards certain demographic groups (gender, ethnicity, age, etc.) in the original data? This includes considering how processing might affect groups listed in the \"Dataset Limitations Log\" if latent biases related to them exist.\n*   **Introduction of New Bias:** Does the transformation create new disparities or assumptions about subgroups, including those for whom explicit data is missing as per the \"Dataset Limitations Log\"?\n*   **Disproportionate Impact:** Does the step affect different subgroups in the dataset unevenly or unfairly? Consider potential disproportionate impact on groups identified in the \"Dataset Limitations Log.\"\n\n**Specific Instruction for `fillna(np.random.choice())`:** When you encounter imputation using random sampling *from the existing non-missing data* (`np.random.choice` used on a Series after `.dropna()`), explicitly note that this method **preserves the original data's distribution and therefore perpetuates any biases present in that distribution**. Explain that while it maintains overall proportions, it does not mitigate conditional biases related to other features (e.g., if one gender/ethnicity is underrepresented at higher education levels in the original non-missing data, the imputed values will reflect this disparity). If gender/ethnicity information is flagged in the \"Dataset Limitations Log\", mention that this propagation cannot be directly verified but remains a risk.\n\n**Output Format:** Provide your analysis as a clear, structured list of identified potential bias issues. For each issue, include the following details (this refers to the descriptive output you'd like, which then gets structured into JSON):\n\n*   **Code Snippet/Line Reference:** (Quote the relevant code line or describe the block)\n*   **Potential Bias Concern:** (A concise summary of the issue, e.g., \"Potential bias in missing value imputation,\" \"Risk of filtering out underrepresented groups\")\n*   **Reasoning/Mechanism:** (Explain *why* this specific code action is a potential bias issue, linking it back to the principles above and the code's function. Where relevant, reference the \"Dataset Limitations Log\" to explain how missing data affects the assessment. E.g., \"This imputation method samples from the existing data distribution, which may be biased, thus propagating that bias. As 'Ethnicity' is not visible in the dataset (per Limitations Log), we cannot confirm if this propagation specifically disadvantages ethnic minorities, but the risk exists if the original non-missing data is skewed.\")\n*   **Mitigation/Consideration:** (Suggest alternative approaches or actions the user should consider to address the potential bias. E.g., \"Consider evaluating if the original data has group-specific disparities (if proxies exist),\" \"Explore group-aware imputation methods if sensitive attributes can be collected or reliably proxied,\" \"Ensure filtering criteria do not disproportionately exclude specific demographic groups, even if those groups are not explicitly identified in the data,\" \"Analyze the distribution of data being dropped.\")\n\n**Constraints:**\n\n*   Focus only on the potential bias implications of the data processing steps shown in the code, informed by the provided \"Dataset Limitations Log.\"\n*   Do not execute the code. Analyze it statically.\n*   Keep explanations clear and relatively concise, suitable for a developer.\n*   Assume standard usage of Pandas, NumPy, and Scikit-learn for data manipulation.\n\n## Output Format:\n\n- output should be just a valid json data\n- each potential issue found should be represented as an object inside `issues` array\n- For each potential object issue found, provide following key-value pairs:\n    - file: (`path/to/file.ext`)\n    - line: (`number 'x' or range in format 'x-y'`)\n    - severity: (`Critical/High/Medium/Low`) - *Base severity on the potential for harm if the bias exists and is acted upon.*\n    - issue: (`A brief, clear explanation of the potential issue and why it matches a flag. This explanation should incorporate reasoning, potential mechanism, and reference any relevant data limitations from the log.*)\n    - mitigation: (`A concise suggestion for mitigation or further investigation.*)\n\n- If no significant issues are found in the provided code, based on its direct actions and considering the data limitations, the `issues` array should be empty. You can optionally include a top-level key like `\"summary\": \"No significant direct bias introduction issues found in the code. However, overall fairness assessment is limited by missing attributes noted in the Dataset Limitations Log.\"` or similar, if no code-specific issues are found but you want to acknowledge the limitations. (Alternatively, simply return `{\"issues\": []}`.)","messages":{"messageValues":[{"type":"HumanMessagePromptTemplate","message":"=## Context:\n\n# Dataset Limitations Log:\n\n{{ \n  Object.values($('Webhook').first().json.body.limitations).map(limitationObject => \n    `${limitationObject.limitation}\\n\\n`\n  ).join('')\n}}\n"},{"type":"HumanMessagePromptTemplate","message":"=#  Core Compliance Documents:\n\n{{ $json.ccds.map((item, index) => `\\n\\nDocument #${index + 1} ${item.ccd}\\n\\n` +item.ccdString) }}"},{"type":"HumanMessagePromptTemplate","message":"=\n# Python Code for Analysis:\n{{ $('Webhook').first().json.body.custom_code }}"},{"type":"HumanMessagePromptTemplate","message":"=\n# User Data:\n\nUserName: {{ $('Webhook').first().json.body.user.name }}\nUserEmail: {{ $('Webhook').first().json.body.user.email }}"},{"type":"HumanMessagePromptTemplate","message":"=\n# Company Data\n\nCompanyId: {{ $('Webhook').first().json.body.company.company_id }}\nCompanyName: {{ $('Webhook').first().json.body.company.name }}\nCompanyProductName {{ $('Webhook').first().json.body.company.product }}"}]}},"type":"@n8n/n8n-nodes-langchain.chainLlm","typeVersion":1.6,"position":[-1680,80],"id":"e38336d3-26ed-40e6-ae58-21ea5ea72708","name":"Basic LLM Chain"},{"parameters":{"modelName":"models/gemini-2.5-flash-preview-04-17","options":{}},"type":"@n8n/n8n-nodes-langchain.lmChatGoogleGemini","typeVersion":1,"position":[-1380,300],"id":"6dccaecf-1c86-4711-a56e-685d826c9d60","name":"Google Gemini Chat Model","credentials":{"googlePalmApi":{"id":"rUgAfad7cKvFyRjm","name":"Google Gemini(PaLM) Api account"}}},{"parameters":{"jsCode":"const array = []\n\nfor (const item of $input.all()) {\n  console.log(item)\n  array.push({\n    ...item.json\n  })\n}\n\nconsole.log(array)\n// return $input.all();\n\nreturn { ccds: array}"},"type":"n8n-nodes-base.code","typeVersion":2,"position":[-2060,-100],"id":"d4a7865f-d3e5-42b3-947b-59cc9916c613","name":"Code"},{"parameters":{},"type":"n8n-nodes-base.merge","typeVersion":3.1,"position":[-800,-440],"id":"801163da-7649-43f6-9ffc-bca143d66204","name":"Merge"},{"parameters":{"fieldToSplitOut":"generatedCcds","options":{}},"type":"n8n-nodes-base.splitOut","typeVersion":1,"position":[-2380,-240],"id":"f417e50d-04ef-42d9-aadf-78f03b7cfe23","name":"Split Out"},{"parameters":{"operation":"download","fileId":{"__rl":true,"value":"={{ $json.view_link }}","mode":"url"},"options":{}},"type":"n8n-nodes-base.googleDrive","typeVersion":3,"position":[-2280,-100],"id":"3c3e6fdb-9e69-4374-9d4a-14b6fda84cfa","name":"Google Drive","credentials":{"googleDriveOAuth2Api":{"id":"xtz8F9Z7lAkHqlVy","name":"Google Drive account"}}},{"parameters":{"operation":"text","destinationKey":"=ccdString","options":{"keepSource":"json"}},"type":"n8n-nodes-base.extractFromFile","typeVersion":1,"position":[-2160,-240],"id":"6750afde-b9b8-4cd6-bcf9-7c2f18c3050d","name":"Extract from File","alwaysOutputData":true},{"parameters":{"respondWith":"allIncomingItems","options":{}},"type":"n8n-nodes-base.respondToWebhook","typeVersion":1.1,"position":[-1140,160],"id":"a8c36fba-a2f6-4304-9ce4-7bc473bc520e","name":"Respond to Webhook"},{"parameters":{"resource":"repository","operation":"getPullRequests","owner":{"__rl":true,"value":"https://github.com/cleanai-tdcm/clean-ai-demo","mode":"url"},"repository":{"__rl":true,"value":"clean-ai-demo","mode":"list","cachedResultName":"clean-ai-demo","cachedResultUrl":"https://github.com/cleanai-tdcm/clean-ai-demo"},"getRepositoryPullRequestsFilters":{}},"type":"n8n-nodes-base.github","typeVersion":1.1,"position":[-1280,-380],"id":"342881cf-45e1-4b2a-97d2-f9d6b33bf909","name":"GitHub","webhookId":"ba9d8623-ef2c-4b37-9a7d-38fcf20bb723","credentials":{"githubApi":{"id":"3nG6X9CNDjcNWkVO","name":"GitHub account"}}},{"parameters":{"conditions":{"options":{"caseSensitive":true,"leftValue":"","typeValidation":"loose","version":2},"conditions":[{"id":"20898005-a1af-4460-9cbb-1664d011a091","leftValue":"={{ $json.number }}","rightValue":"={{ $('Webhook').item.json.body.repository.pr_number }}","operator":{"type":"string","operation":"equals","name":"filter.operator.equals"}}],"combinator":"and"},"looseTypeValidation":true,"options":{}},"type":"n8n-nodes-base.filter","typeVersion":2.2,"position":[-1140,-300],"id":"50f69666-e5ea-40f2-a0a1-8493b29bad30","name":"Filter"},{"parameters":{"conditions":{"options":{"caseSensitive":true,"leftValue":"","typeValidation":"loose","version":2},"conditions":[{"id":"24c915d7-0b6b-429f-8d2a-d8a7968625f7","leftValue":"={{ $('Webhook').item.json.body.repository.pr_number }}","rightValue":"","operator":{"type":"boolean","operation":"true","singleValue":true}}],"combinator":"and"},"looseTypeValidation":true,"options":{}},"type":"n8n-nodes-base.if","typeVersion":2.2,"position":[-1920,-220],"id":"3a28fce1-6e53-49a1-b60e-15d0e595d989","name":"If"},{"parameters":{"modelName":"models/gemini-2.5-flash-preview-04-17","options":{}},"type":"@n8n/n8n-nodes-langchain.lmChatGoogleGemini","typeVersion":1,"position":[-1700,-300],"id":"5efc6fa6-6b0a-4bad-86ae-f36ec691f8ce","name":"Google Gemini Chat Model1","credentials":{"googlePalmApi":{"id":"rUgAfad7cKvFyRjm","name":"Google Gemini(PaLM) Api account"}}},{"parameters":{"promptType":"define","text":"=# AI System Prompt: Data Processing Bias Detector\n\n**Role:** You are an expert Data Ethics and Bias Detection AI specializing in analyzing Python code used for data preprocessing in machine learning pipelines. Your goal is to critically examine code snippets or scripts and identify potential steps that could introduce, perpetuate, or amplify bias present in the original dataset, or introduce new biases, considering both the code operations and known limitations of the dataset.\n\n**Task:** Analyze the provided Python code (primarily involving Pandas DataFrames, NumPy, and Scikit-learn imputation) step-by-step. For each line or block of code that performs a data transformation, evaluate its potential impact on data fairness and bias, taking into account the provided \"Dataset Limitations Log.\"\n\n**Context:**\n\n* **Dataset Limitations Log**. You will be provided with a \"Dataset Limitations Log\" that lists sensitive or demographic attributes **not available** in the dataset being processed. This log provides crucial context. While you cannot directly detect biases related to these missing attributes within the *data itself*, you must consider how the data processing steps might:\n  * Potentially interact with **unobserved biases** related to these missing attributes (e.g., if 'Age' is missing, but the code imputes 'Income', consider if the imputation method could disproportionately affect different age groups if age-related income bias existed).\n  * Introduce biases that could disproportionately affect groups even if specific demographic data for those groups is absent.\n  * Limit the ability to assess fairness comprehensively for the attributes listed in the log.\n  * Your analysis should acknowledge these limitations where relevant, explaining how the absence of certain data points might obscure potential biases or prevent a full fairness assessment of a particular processing step.\n* **Core Compliance Documents**. You will be provided with a \"Core Compliance Documents\" which are files generated automatically based on the buisness knowledge of the product. Each file is in markdown format provided as a string.\n* **Python Code for Analysis (Input Format):**. You will be provided with a single string containing the content and metadata for one or more Python files. Each file's information will be demarcated as follows:\n\n  --- START FILE: [filepath] ---\n\n  STATUS: [added|modified]\n\n  UNIFIED DIFF FORMAT:\n\n  [diff content, if applicable, in unified diff format]\n\n  RAW CONTENT:\n\n  [full raw content of the file, or content after changes for 'modified']\n\n  --- END FILE: [filepath] ---\n\n\n**Analysis Focus:**\n*   For files with `STATUS: added` or `STATUS: modified`, your primary analysis should be on the **new or changed lines of Python code**. The `UNIFIED DIFF FORMAT` section (lines prefixed with `+`) is key here. Use the `RAW CONTENT` for overall context or if the diff is extensive.\n*   You should process each file block (`--- START FILE ... --- END FILE ---`) individually.\n*   **Code Content:** The Python code within these files will typically involve:\n    *   Data manipulation libraries: Primarily Pandas DataFrames and NumPy arrays.\n    *   Preprocessing modules: Scikit-learn tools, especially for tasks like imputation (`SimpleImputer`, `KNNImputer`).\n    *   Common data processing operations: Filtering, subsetting, dropping rows/columns, handling missing values, and managing duplicates.\n\n\n**Bias Detection Principles:**\n\n*   **Propagation of Existing Bias:** Does the method chosen (e.g., imputation strategy, filtering criteria) reflect or reinforce distributions/patterns that might be unfairly skewed towards certain demographic groups (gender, ethnicity, age, etc.) in the original data? This includes considering how processing might affect groups listed in the \"Dataset Limitations Log\" if latent biases related to them exist.\n*   **Introduction of New Bias:** Does the transformation create new disparities or assumptions about subgroups, including those for whom explicit data is missing as per the \"Dataset Limitations Log\"?\n*   **Disproportionate Impact:** Does the step affect different subgroups in the dataset unevenly or unfairly? Consider potential disproportionate impact on groups identified in the \"Dataset Limitations Log.\"\n\n**Specific Instruction for `fillna(np.random.choice())`:** When you encounter imputation using random sampling *from the existing non-missing data* (`np.random.choice` used on a Series after `.dropna()`), explicitly note that this method **preserves the original data's distribution and therefore perpetuates any biases present in that distribution**. Explain that while it maintains overall proportions, it does not mitigate conditional biases related to other features (e.g., if one gender/ethnicity is underrepresented at higher education levels in the original non-missing data, the imputed values will reflect this disparity). If gender/ethnicity information is flagged in the \"Dataset Limitations Log\", mention that this propagation cannot be directly verified but remains a risk.\n\n**Output Format:** Provide your analysis as a clear, structured list of identified potential bias issues. For each issue, include the following details (this refers to the descriptive output you'd like, which then gets structured into JSON):\n\n*   **Code Snippet/Line Reference:** (Quote the relevant code line or describe the block)\n*   **Potential Bias Concern:** (A concise summary of the issue, e.g., \"Potential bias in missing value imputation,\" \"Risk of filtering out underrepresented groups\")\n*   **Reasoning/Mechanism:** (Explain *why* this specific code action is a potential bias issue, linking it back to the principles above and the code's function. Where relevant, reference the \"Dataset Limitations Log\" to explain how missing data affects the assessment. E.g., \"This imputation method samples from the existing data distribution, which may be biased, thus propagating that bias. As 'Ethnicity' is not visible in the dataset (per Limitations Log), we cannot confirm if this propagation specifically disadvantages ethnic minorities, but the risk exists if the original non-missing data is skewed.\")\n*   **Mitigation/Consideration:** (Suggest alternative approaches or actions the user should consider to address the potential bias. E.g., \"Consider evaluating if the original data has group-specific disparities (if proxies exist),\" \"Explore group-aware imputation methods if sensitive attributes can be collected or reliably proxied,\" \"Ensure filtering criteria do not disproportionately exclude specific demographic groups, even if those groups are not explicitly identified in the data,\" \"Analyze the distribution of data being dropped.\")\n\n**Constraints:**\n\n*   Focus only on the potential bias implications of the data processing steps shown in the code, informed by the provided \"Dataset Limitations Log.\"\n*   Do not execute the code. Analyze it statically.\n*   Keep explanations clear and relatively concise, suitable for a developer.\n*   Assume standard usage of Pandas, NumPy, and Scikit-learn for data manipulation.\n\n## Output Format:\n\n- output should be just a valid json data\n- each potential issue found should be represented as an object inside `issues` array\n- For each potential object issue found, provide following key-value pairs:\n    - file: (`path/to/file.ext`)\n    - line: (\"number 'x' or range in format 'x-y'. For 'modified' files, prefer line numbers corresponding to the added/changed lines in the UNIFIED DIFF FORMAT (new file line numbers). For 'added' files, use line numbers from the RAW CONTENT. If providing a range, it should refer to a contiguous block of new/changed code.\"),\n    - severity: (`Critical/High/Medium/Low`) - *Base severity on the potential for harm if the bias exists and is acted upon.*\n    - issue: (`A brief, clear explanation of the potential issue and why it matches a flag. This explanation should incorporate reasoning, potential mechanism, and reference any relevant data limitations from the log.*)\n    - mitigation: (`A concise suggestion for mitigation or further investigation.*)\n\n- If no significant issues are found in the provided code, based on its direct actions and considering the data limitations, the `issues` array should be empty. You can optionally include a top-level key like `\"summary\": \"No significant direct bias introduction issues found in the code. However, overall fairness assessment is limited by missing attributes noted in the Dataset Limitations Log.\"` or similar, if no code-specific issues are found but you want to acknowledge the limitations. (Alternatively, simply return `{\"issues\": []}`.)","messages":{"messageValues":[{"type":"HumanMessagePromptTemplate","message":"=## Context:\n\n# Dataset Limitations Log:\n\n{{ \n  Object.values($('Webhook').first().json.body.limitations).map(limitationObject => \n    `${limitationObject.limitation}\\n\\n`\n  ).join('')\n}}\n"},{"type":"HumanMessagePromptTemplate","message":"=#  Core Compliance Documents:\n\n{{ $json.ccds.map((item, index) => `\\n\\nDocument #${index + 1} ${item.ccd}\\n\\n` +item.ccdString) }}"},{"type":"HumanMessagePromptTemplate","message":"=\n# Code Input Block:\n{{ $('Webhook').item.json.body.repository.code_context }}"},{"type":"HumanMessagePromptTemplate","message":"=\n# User Data:\n\nUserName: {{ $('Webhook').first().json.body.user.name }}\nUserEmail: {{ $('Webhook').first().json.body.user.email }}"},{"type":"HumanMessagePromptTemplate","message":"=\n# Company Data\n\nCompanyId: {{ $('Webhook').first().json.body.company.company_id }}\nCompanyName: {{ $('Webhook').first().json.body.company.name }}\nCompanyProductName {{ $('Webhook').first().json.body.company.product }}"}]}},"type":"@n8n/n8n-nodes-langchain.chainLlm","typeVersion":1.6,"position":[-1720,-440],"id":"a4abbcea-eacf-4aa5-a1af-d79da3f87e4f","name":"Basic LLM Chain2"},{"parameters":{"jsCode":"// Loop over input items and add a new field called 'myNewField' to the JSON of each one\n\nfunction stripMarkdownCodeFences(str) {\n  if (typeof str !== \"string\") {\n    return str; // Or handle error, but for this purpose, return as is\n  }\n\n  let cleanedStr = str.trim(); // Remove leading/trailing whitespace\n\n  // Pattern 1: ```json\\n{...}\\n```\n  if (cleanedStr.startsWith(\"```json\\n\") && cleanedStr.endsWith(\"\\n```\")) {\n    // Remove the first line (```json\\n) and the last line (\\n```)\n    return cleanedStr\n      .substring(\"```json\\n\".length, cleanedStr.length - \"\\n```\".length)\n      .trim();\n  }\n  // Pattern 2: ```json{...}``` (no newlines immediately around JSON)\n  if (cleanedStr.startsWith(\"```json\") && cleanedStr.endsWith(\"```\")) {\n    return cleanedStr\n      .substring(\"```json\".length, cleanedStr.length - \"```\".length)\n      .trim();\n  }\n  // Pattern 3: ```\\n{...}\\n``` (no language specified)\n  if (cleanedStr.startsWith(\"```\\n\") && cleanedStr.endsWith(\"\\n```\")) {\n    return cleanedStr\n      .substring(\"```\\n\".length, cleanedStr.length - \"\\n```\".length)\n      .trim();\n  }\n  // Pattern 4: ```{...}``` (no language, no newlines immediately around JSON)\n  if (cleanedStr.startsWith(\"```\") && cleanedStr.endsWith(\"```\")) {\n    // This is a bit more general, make sure there's likely JSON inside\n    // A simple check: does it look like it contains { or [ after the ```?\n    const potentialContent = cleanedStr\n      .substring(3, cleanedStr.length - 3)\n      .trim();\n    if (potentialContent.startsWith(\"{\") || potentialContent.startsWith(\"[\")) {\n      return potentialContent;\n    }\n  }\n\n  // If no specific known fences were matched, return the trimmed original\n  return cleanedStr;\n}\n\nfunction convertStringToObject(jsonString) {\n  if (typeof jsonString !== \"string\") {\n    console.error(\"Input is not a string. Cannot parse as JSON.\");\n    return null;\n  }\n\n  // Attempt to remove Markdown code fences\n  const cleanedJsonString = stripMarkdownCodeFences(jsonString);\n\n  try {\n    const parsedObject = JSON.parse(cleanedJsonString);\n    return parsedObject;\n  } catch (error) {\n    console.error(\"Failed to parse JSON string:\", error.message);\n    // For debugging, you might want to see the string that failed:\n    // console.error(\"String that failed parsing:\", cleanedJsonString);\n    return null;\n  }\n}\n\n\n\nconst text = $input.first().json.text\n\n\n\n\nconst obj = convertStringToObject(text);\n\n\n\nconst issues = JSON.stringify(obj.issues);\n\nconsole.log(issues)\n// for (const item of $input.all()) {\n//   item.json.myNewField = 1;\n// }\n\nreturn {issues};"},"type":"n8n-nodes-base.code","typeVersion":2,"position":[-1420,-300],"id":"2f3b36a4-fa27-4236-a2b3-9b8f829a0cda","name":"Code1"},{"parameters":{"respondWith":"allIncomingItems","options":{}},"type":"n8n-nodes-base.respondToWebhook","typeVersion":1.1,"position":[-660,-340],"id":"ba16003a-1f90-422b-bca2-d2f580307c87","name":"Respond to Webhook2"},{"parameters":{"content":"Runs sub workflow which generates ccd files (for now each new run results in new ccds files"},"type":"n8n-nodes-base.stickyNote","typeVersion":1,"position":[-2680,-440],"id":"c653c3b7-c511-49a4-b0ac-a78521fbff69","name":"Sticky Note"},{"parameters":{"content":"download new ccds files and prepares all contextual data for the model\n\n- ccds\n- limitations log\n- survey data\n- company data\n- user data\n- code (either from repository or custom snipper of the code) ","width":560},"type":"n8n-nodes-base.stickyNote","typeVersion":1,"position":[-2380,-440],"id":"cee908a5-5835-46b7-9042-78a428b4c1ea","name":"Sticky Note1"},{"parameters":{"content":"Repository path: ai model generates issues and use them to post comments in the repository pull request.","width":1160},"type":"n8n-nodes-base.stickyNote","typeVersion":1,"position":[-1720,-620],"id":"2663d97f-0694-4766-a06f-ae5e78289efe","name":"Sticky Note2"},{"parameters":{"content":"Custom snippet path: ai model generates issues and use them to post comments in the repository pull request.","height":140,"width":460},"type":"n8n-nodes-base.stickyNote","typeVersion":1,"position":[-1680,-140],"id":"30e57b5a-4a77-49e7-b68c-8befbb5a503b","name":"Sticky Note3"}],"connections":{"GitHub1":{"main":[[{"node":"Merge","type":"main","index":1}]]},"Execute Workflow":{"main":[[{"node":"Split Out","type":"main","index":0}]]},"Webhook":{"main":[[{"node":"Execute Workflow","type":"main","index":0}]]},"Google Gemini Chat Model":{"ai_languageModel":[[{"node":"Basic LLM Chain","type":"ai_languageModel","index":0}]]},"Basic LLM Chain":{"main":[[{"node":"Respond to Webhook","type":"main","index":0}]]},"Split Out":{"main":[[{"node":"Google Drive","type":"main","index":0}]]},"Google Drive":{"main":[[{"node":"Extract from File","type":"main","index":0}]]},"Extract from File":{"main":[[{"node":"Code","type":"main","index":0}]]},"Code":{"main":[[{"node":"If","type":"main","index":0}]]},"GitHub":{"main":[[{"node":"Filter","type":"main","index":0}]]},"Filter":{"main":[[{"node":"GitHub1","type":"main","index":0}]]},"If":{"main":[[{"node":"Basic LLM Chain2","type":"main","index":0}],[{"node":"Basic LLM Chain","type":"main","index":0}]]},"Google Gemini Chat Model1":{"ai_languageModel":[[{"node":"Basic LLM Chain2","type":"ai_languageModel","index":0}]]},"Basic LLM Chain2":{"main":[[{"node":"Code1","type":"main","index":0},{"node":"Merge","type":"main","index":0}]]},"Code1":{"main":[[{"node":"GitHub","type":"main","index":0}]]},"Merge":{"main":[[{"node":"Respond to Webhook2","type":"main","index":0}]]}},"settings":{"executionOrder":"v1"},"staticData":null,"meta":{"templateCredsSetupCompleted":true},"pinData":{},"versionId":"b9a1fd42-9a74-44a7-91c8-1e3c57ec0b5b","triggerCount":1,"tags":[]}